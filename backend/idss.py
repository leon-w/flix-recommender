import os
import pickle
import time
from functools import lru_cache

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from movie_metadata import get_metadata

METADATA = get_metadata(key='title')


def get_df_size(df):
    b = df.memory_usage(index=True).sum()
    return f'{b / (1024 * 1024):.3f} MB'


class RecommenderSystem():
    # Initialize Recommender
    def __init__(self, cache_file='data/recommender_cache.pickle'):
        if not os.path.exists(cache_file):
            print('Computing pivot table...')
            # we load the preprocessed file generated by the `data_parser` and turn it into a DataFrame later
            preprocessed_reviews_type = np.dtype([('movie_id', '>u2'),
                                                  ('customer_id', '>u4'),
                                                  ('rating', '>u1')])
            data = np.fromfile('data/preprocessed_reviews_all.bin', dtype=preprocessed_reviews_type)
            df = pd.DataFrame(data)

            # calculate the indices of 70 percentile of the most relevant movies/customers
            self.df_movie_summary = df.groupby('movie_id')['rating'].agg(['count'])
            df_cust_summary = df.groupby('customer_id')['rating'].agg(['count'])
            movie_benchmark = self.df_movie_summary['count'].quantile(0.7)
            cust_benchmark = df_cust_summary['count'].quantile(0.7)
            drop_movie_list = self.df_movie_summary[self.df_movie_summary['count'] < movie_benchmark].index
            drop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index

            # filter out reviews of irrelevant movies/customers
            df = df[~df['movie_id'].isin(drop_movie_list)]
            df = df[~df['customer_id'].isin(drop_cust_list)]

            # Let's pivot the data set and put it into a giant matrix - we need it for our recommendation system:
            self.df_p = pd.pivot_table(df, values='rating', index='customer_id', columns='movie_id')

            # cache the result since the calculation is slow
            with open(cache_file, 'wb') as f:
                pickle.dump({
                    'df_p': self.df_p,
                    'df_movie_summary': self.df_movie_summary,
                }, f)
        else:
            print('Using cached pivot table...')
            # load the previously calculated dataframes
            with open(cache_file, 'rb') as f:
                cache = pickle.load(f)
                self.df_p = cache['df_p']
                self.df_movie_summary = cache['df_movie_summary']

        self.df_title = pd.read_csv('data/movie_titles.csv', encoding="ISO-8859-1",
                                    header=None, names=['movie_id', 'year', 'movie_name'])
        self.df_title.set_index('movie_id', inplace=True)

    def explore_dataset(self, df):
        print('Full dataset shape: {}'.format(df.shape))
        print('-Dataset examples-')
        print(df.iloc[::5000000, :])

        p = df.groupby('Rating')['Rating'].agg(['count'])

        # get movie count
        movie_count = df.isnull().sum()[1]

        # get customer count
        cust_count = df['Cust_Id'].nunique() - movie_count

        rating_count = df['Cust_Id'].count() - movie_count
        ax = p.plot(kind='barh', legend=False, figsize=(15, 10))
        plt.title(
            'Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count),
            fontsize=20)
        plt.axis('off')
        for i in range(1, 6):
            ax.text(p.iloc[i - 1][0] / 4, i - 1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i - 1][0] * 100 / p.sum()[0]),
                    color='white', weight='bold')
        plt.show()

        # print('Movie minimum times of review: {}'.format(movie_benchmark))

        # print('Customer minimum times of review: {}'.format(cust_benchmark))
        print('Original Shape: {}'.format(df.shape))
        print('After Trim Shape: {}'.format(df.shape))
        print('-Data Examples-')
        print(df.iloc[::5000000, :])

    @lru_cache()
    def recommend(self, movie_id, min_count=0, limit=10):
        # Recommend with Pearsons' R correlations
        # The way it works is we use Pearsons' R correlation to measure the linear correlation
        # between review scores of all pairs of movies, then we provide the top `limit` movies
        # with highest correlations:

        time.sleep(0.5)

        try:
            print('Recommending for movie id', movie_id)
            target = self.df_p[int(movie_id)]
        except KeyError:
            # no recommendations for movie
            return None

        similar_to_target = self.df_p.corrwith(target)
        corr_target = pd.DataFrame(similar_to_target, columns=['PearsonR'])
        corr_target.dropna(inplace=True)
        corr_target = corr_target.sort_values('PearsonR', ascending=False)
        corr_target = corr_target.join(self.df_title).join(self.df_movie_summary)
        corr_target = corr_target.iloc[1:]  # remove input movie from recommendation

        # filter to at least `min_count` reviews and at most `limit` results in total
        corr_target = corr_target[corr_target['count'] >= min_count][:limit]

        result_dict = corr_target[['PearsonR', 'movie_name']].to_dict(orient='records')
        for r in result_dict:
            r['metadata'] = METADATA[r['movie_name']]

        return result_dict
